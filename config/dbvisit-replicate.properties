##
# Copyright 2016 Dbvisit Software Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##

################################################################################
#                         Static configuration                                 #
################################################################################
#
# configuration name
name=dbvisit-replicate
# internal project version number
project.version=2.9.00
# replicate source connector class to start
connector.class=com.dbvisit.replicate.kafkaconnect.ReplicateSourceConnector
#

################################################################################
#                            User configuration                                #
################################################################################
#
# maximum number of tasks to start for processing PLOGs
tasks.max=4
#
# prefix for all topic names, defaults to 'REP-'.
# NOTE: do not use underscores
# topic.prefix=REP-

#
# replicate PLOG location URI, must be shared location and accessible by all
# kafka connect workers
plog.location.uri=file:/home/oracle/ktest/mine
#
# LCRs to keep before flushing, for connector this is the batch size, choose
# this value according to transaction/network traffic, defaults to 1000
#plog.data.flush.size=1000

#
# time in milliseconds for one wait interval, used by scans and health check,
# defaults to 500ms
#plog.interval.time.ms=500

#
# number of intervals between scans, defaults to 5
# e.g. 5 x 0.5s = 2.5s scan wait time
#plog.scan.interval.count=5

#
# number of intervals between health checks, these are used when initially
# waiting for MINE to produce PLOGs, defaults to 10
# eg. 10 * 0.5s = 5.0s
#plog.health.check.interval=10

#
# default number of health check scans to decide whether or not replicate 
# is offline, this is used as time out value, default to 1000
#
# for testing use 1 - quit after first health check
# 1 * 10 * 0.5s = 5s 
# where 10   is plog.health.check.interval value
#   and 0.5s is plog.interval.time.ms value
#plog.scan.offline.interval=1000

#
# topic name for transaction meta data stream, defaults to 'TX.META'
# NOTE: topic prefix will be applied to transaction topic name to ensure its unique
#topic.name.transaction.info=TX.META

#
# Global SCN when to start loading data during cold start, default to 0 (all records)
#plog.global.scn.cold.start=0

#
# Set the output CDC format of the replicate connector, this determines what type
# of messages are published to Kafka, defaults to 'changerow'
# The options are: 
#   changerow - complete row, the view of the table record after the action was applied
#   changeset - publish the KEY, NEW, OLD and LOB change fields separately
#connector.publish.cdc.format=changerow

#
# Set whether or not the transaction info topic should be populated, this includes
# adding three fields to each record XID (transaction ID), TYPE (type of change) and 
# CHANGE_ID (unique ID) when set to true. If not needed, set the false, defaults
# to true
#connector.publish.transaction.info=true

#
# Set whether or not keys should be published to all table topics. Keys are
# either primary or unique table constraints, when none of these are available
# all columns with either character, numeric or date data types are used as 
# key. The latter is not ideal, so it is encouraged to use PK or unique key
# constraints on table. The default is false, as in, do not publish keys.
#connector.publish.keys=false

#
# If logical data types are used as default values certain versions of
# SchemaRegistry might fail validation due to an issue, see #556. This option is
# provided for disabling schema evolution for BACKWARDS compatible schemas,
# default to true
#connector.publish.no.schema.evolution=true

#
# Define the source schemas, as comma separated list of fully qualified
# source table name,  that may be considered static or only receiving 
# sporadic updates. The committed offsets of their last message can be 
# safely ignored if the lapsed days between the source PLOG of a new 
# message and that of a previous one is more than 'topic.static.offsets.age.days'
#topic.static.schemas=SCHEMA.TABLE1,SCHEMA.TABLE2

#
# The age of the last committed offset for a static schema 
# ('topic.static.schemas'), when it can be safely ignored during
# a task restart and stream rewind. A message that originated
# from a source PLOG older will be considered static and not
# restart at its original source PLOG stream offset, but instead
# at its next available message offset. This is intended for
# static look up tables that rarely change when their source PLOGs
# may have been flushed since their last update, defaults to 7 days
#topic.static.offsets.age.days=7

#
# The kafka brokers to use for storing internal catalog records, defaults to 
# 'localhost:9092'
#connector.catalog.bootstrap.servers=localhost:9092

#
# The topic name for the internal catalog topic to use, useful for monitoring
# replication, defaults to 'replicate-info'
#connector.catalog.topic.name=replicate-info

