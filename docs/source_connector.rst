Replicate Connector for Kafka
==============================

The **Replicate Connector for Kafka** enables you to stream DML change data (inserts, deletes, updates) for an Oracle database, which has been made available in PLOG (parsed log) files generated by the Dbvisit Replicate application, into Kafka topics, via the Kafka Connect framework.

Change data is identified and captured from the Oracle RDBMS by the Dbvisit Replicate application, using proprietary technology to mine the online redo logs in real time. This is streamed, in a custom binary format called a PLOG (parsed log) file, to a location where the Replicate Connector for Kafka picks this up, processes it and delivers to Kafka topics.

Only changes made to tables or schemas we are interested in listening for on the Oracle database are delivered through, and the default mapping is that each table has changes written to its own (automatically generated) topic in Kafka. 

Pessimistic commit mode...

New tables if limited DDL replication has been enabled on the source...
In short, with the Dbvisit Replicate Connector for Kafka we automatically include a meta data topic which lists out the Transactions (TX), and an ordered list of the changes contained with these, which can be utilized/cross- 

Quickstart
----------
To demonstrate the operation and functionality of the connector, we have provided an example set of PLOG files, generated by running the `Swingbench <http://dominicgiles.com/swingbench.html>`_ load generation utility, against an Oracle XE 11g database. These can be downloaded from here (or should they go into an install package?). Once the Kafka processes and the Replicate Connector itself have been started up, then it will ingest and process these PLOG files, writing the change data record messages to Kafka â€“ and which can be viewed on the other side with a consumer.

Download the Confluent Platform version 2+ from here, and install onto your test server. Ie: /usr/confluent (NB: the Java and platform prequisites here).

Download the Replicate Connector JAR file from here, and install into the location <>.

Download the Replicate Connector QuickStart properties file from here and install into this location <>. More details on the configuration specifics can be found later in this document.

Start the Zookeeper, Kafka and Schema Registry (and optionally the REST Proxy) services in the background on your test server. You can use the following commands to start these processes, or put them in a script for convenience (ie: kafka-init.sh):
    
    #! /bin/bash
    #
    # should run these as daemons!
    
    echo $(hostname)
    CONFLUENT_HOME=/usr/confluent/confluent-3.0.0
    
    echo "INFO Starting Zookeeper"
    $CONFLUENT_HOME/bin/zookeeper-server-start -daemon $CONFLUENT_HOME/etc/kafka/zookeeper.properties
    sleep 10
    
    echo "INFO Starting Kafka Server"
    $CONFLUENT_HOME/bin/kafka-server-start -daemon $CONFLUENT_HOME/etc/kafka/server.properties
    sleep 10
    
    echo "INFO Starting Schema Registry"
    $CONFLUENT_HOME/bin/schema-registry-start -daemon $CONFLUENT_HOME/etc/schema-registry/schema-registry.properties
    #sleep 10
    
    echo "INFO Starting REST Proxy"
    $CONFLUENT_HOME/bin/kafka-rest-start -daemon $CONFLUENT_HOME/etc/kafka-rest/kafka-rest.properties
    sleep 10

Now, run the connector in a standalone Kafka Connect worker in another terminal (this assumes Avro settings and that Kafka and the Schema Registry are running locally on the default ports):
example

You should see the process start up and log some messages, and then it will begin executing queries and sending the results to Kafka. In order to check that it has copied the data that was present when we started Kafka Connect, start a console consumer, reading from the beginning of the topic:
    $ ./bin/kafka-avro-console-consumer --new-consumer --bootstrap-server localhost:9092 --topic test-sqlite-jdbc-accounts --from-beginning {"id":1,"name":{"string":"alice"}} {"id":2,"name":{"string":"bob"}} 

The output shows the two records as expected, one per line, in the JSON encoding of the Avro records. Each row is represented as an Avro record and each column is a field in the record. We can see both columns in the table, idand name. The IDs were auto-generated and the column is of type INTEGER NOT NULL, which can be encoded directly as an integer. The name column has type STRING and can be NULL. The JSON encoding of Avro encodes the strings in the format {"type": value}, so we can see that both rows have string values with the names specified when we inserted the data.

Of course, all the features of Kafka Connect, including offset management and fault tolerance, work with the Replicate Connector for Kafka. You can restart and kill the processes and they will pick up where they left off, copying only new data (as defined by the mode setting).

Features
--------

Metadata Topic

In short, with the Dbvisit Replicate Connector for Kafka we automatically include a meta data topic which lists out the Transactions (TX), and an ordered list of the changes contained with these, which can be utilized/cross-referenced within consumers to reconstruct change ordering across different tables (and manifested in different topics).

The transaction info/meta data topic contains the following fields:

- XID - transaction ID (as per data message)
- START/END SCN - start and end SCN of transaction
- START/END CHANGE ID - start and end id of LCRs in transaction
- CHANGE_COUNT - the number of change records, this is the count of data records and corresponds to the total number of messages published to kakfa for this transaction

So the output of a TX meta data record will look like this:  *{"XID":"0004.00c.0000012c","START_SCN":407622,"END_SCN":407628,"START_TIME":1470259744000,"END_TIME":1470259744000,"START_CHANGE_ID":9010647623,"END_CHANGE_ID":9010667623,"CHANGE_COUNT":20000}*



Configuration
-------------



Examples
~~~~~~~~



Schema Evolution
----------------


